{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Support Vector Machines.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNq5TCEdR4FH6oeesKyrDJp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/easypanda/Handson-ML2/blob/master/Support_Vector_Machines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDgyowoK2GV4",
        "colab_type": "text"
      },
      "source": [
        "#Support Vector Machines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09Bz-S4K1jFq",
        "colab_type": "text"
      },
      "source": [
        "A Support Vector Machine (SVM) is a powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression , and even outlier detection. SVMs are particulary well suited for classification of complex small -or medium- sized datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2-5qrqX2LDC",
        "colab_type": "text"
      },
      "source": [
        "## Linear SVM Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-fABYsD2oHo",
        "colab_type": "text"
      },
      "source": [
        "You can think of an SVM classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes. This is called **large margin classification**. Notice that adding more training instances \"off the street\" will not affect the decision boundary at all: it is fully determined by the instances located on the edge of the street. These instances are called the **Support Vectors**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_kdbwIr3sFP",
        "colab_type": "text"
      },
      "source": [
        "/!\\ : SVMs are sensible to the feature scales so we should do feature scaling each time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwFqD-ou36CU",
        "colab_type": "text"
      },
      "source": [
        "## Soft Margin Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-oBxRMg4s6k",
        "colab_type": "text"
      },
      "source": [
        "If we strictly impose that all instances must be off the street and on the right side, this is called **hard margin classification**.\n",
        "This has two main issues:\n",
        "* It only works if the data are linearly seperable.\n",
        "* It is sensitive to outliers.\n",
        "\n",
        "We choose use a more flexible model that ensure the good balance between keeping the street as large as possible and limiting the *margin violations*.\n",
        "This is called **Soft Margin Classification**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l-bpgqK5ykc",
        "colab_type": "text"
      },
      "source": [
        "When creating a model with Scikit-Learn, we can specify the number of hyperparameters including C. if C is set to a low value, we enable a large margin and therefore more margin violations whereas with a high C, a fewer margin violations. Sometimes it's more sutable to have more margin violations to ensure a better generalization of the model.\n",
        "\n",
        "/!\\ If the model is overfitting, we can try regularizing by reducing C."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uV1WOs0D25G1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvYP8SG_6vqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iris = datasets.load_iris()\n",
        "X = iris[\"data\"][:,(2,3)] #Petal length and petal width columns\n",
        "y = (iris[\"target\"] == 2).astype(np.float64) #Just Iris virginica"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWmEeboq7F3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svm_clf = Pipeline([\n",
        "                    (\"scaler\",StandardScaler()),\n",
        "                    (\"linear_svc\",LinearSVC(C=1,loss=\"hinge\")),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MBwnNPH7QT2",
        "colab_type": "code",
        "outputId": "501f63b8-a497-434f-a019-3645afde3e15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "svm_clf.fit(X,y)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('scaler',\n",
              "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
              "                ('linear_svc',\n",
              "                 LinearSVC(C=1, class_weight=None, dual=True,\n",
              "                           fit_intercept=True, intercept_scaling=1,\n",
              "                           loss='hinge', max_iter=1000, multi_class='ovr',\n",
              "                           penalty='l2', random_state=None, tol=0.0001,\n",
              "                           verbose=0))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqnwhF6Z7Sxf",
        "colab_type": "code",
        "outputId": "878d95a6-1e74-4a73-a97f-089f417c194e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "svm_clf.predict([[5.5,1.7]])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2YTs1ce7Zt1",
        "colab_type": "text"
      },
      "source": [
        "**/!\\** Unline Logistic Regression, SVM does not output probabilities for each class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKnnmew39RIR",
        "colab_type": "text"
      },
      "source": [
        "We could also use the SVC class with a liner kernel (SVC(kernel=\"linear\",C=1)) or with the SGDClassifier (SGDClassifier(loss=\"hinge\",alpha=1/(m*c))). It does not converge as fast as the LinearSVC class but it can be useful to handle online classification tasks or huge datasets that do not fit in memory (out-of-core training).\n",
        "\n",
        "Loss hyperparameter should be on \"hinge\" and for better performance, dual hyperparameter should be set on False."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbbpZZGn9-Oq",
        "colab_type": "text"
      },
      "source": [
        "## Nonlinear SVM Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7WJugV9-WE4",
        "colab_type": "text"
      },
      "source": [
        "One approche to handle nonlinear datasets is to add more features, such as polynomial features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBDPFBEk7W6o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TROb5quD-fdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X,y = make_moons(n_samples=100,noise=0.15)\n",
        "polynomial_svm_clf = Pipeline([\n",
        "                               ('poly_features',PolynomialFeatures(degree=3)),\n",
        "                               ('scaler',StandardScaler()),\n",
        "                               ('svm_clf',LinearSVC(C=10,loss=\"hinge\"))\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRQnRwIM_P2u",
        "colab_type": "code",
        "outputId": "9850f9d1-c8c5-4035-96c4-ba55cb238593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "polynomial_svm_clf.fit(X,y)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('poly_features',\n",
              "                 PolynomialFeatures(degree=3, include_bias=True,\n",
              "                                    interaction_only=False, order='C')),\n",
              "                ('scaler',\n",
              "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
              "                ('svm_clf',\n",
              "                 LinearSVC(C=10, class_weight=None, dual=True,\n",
              "                           fit_intercept=True, intercept_scaling=1,\n",
              "                           loss='hinge', max_iter=1000, multi_class='ovr',\n",
              "                           penalty='l2', random_state=None, tol=0.0001,\n",
              "                           verbose=0))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZdZSsIk_V1P",
        "colab_type": "text"
      },
      "source": [
        "## Polynomial Kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzdNdCNS_9ux",
        "colab_type": "text"
      },
      "source": [
        "Adding a polynomial features is simple to implement and can work great with all sorts of Machine Learning algorithms (not just SVM). That said, at a low polynomial degree, this method cannot deal with very complex datasets, and with a high polynomial degree it creates a huge number of features, making the model too slow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he-cdHouABwh",
        "colab_type": "text"
      },
      "source": [
        "But with SVM, you can apply a technique called **kernel trick** to make possible to have the same result as if you had added many polynomial features, even with very high-degree polynomials without actually adding them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFgTdB1y_STf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2JzZ_MvAXOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "poly_kernel_svm_clf = Pipeline([\n",
        "                                (\"scaler\",StandardScaler()),\n",
        "                                (\"svm_clf\",SVC(kernel=\"poly\",degree=3,coef0=1,C=5))\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDiWl2p-Alrf",
        "colab_type": "code",
        "outputId": "58786ef1-ba29-4f63-dd9d-cb32d377769b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "poly_kernel_svm_clf.fit(X,y)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('scaler',\n",
              "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
              "                ('svm_clf',\n",
              "                 SVC(C=5, break_ties=False, cache_size=200, class_weight=None,\n",
              "                     coef0=1, decision_function_shape='ovr', degree=3,\n",
              "                     gamma='scale', kernel='poly', max_iter=-1,\n",
              "                     probability=False, random_state=None, shrinking=True,\n",
              "                     tol=0.001, verbose=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIB55tBJBOFV",
        "colab_type": "text"
      },
      "source": [
        "This SVM classifier uses a third-degree polynomial kernel.\n",
        "If the model is overfitting, we can decrease the polynomial degree and if it's underfitting we can increase it.\n",
        "The hyperparemeter coef0 controls how much the model is unfluenced by high-degree polynomials versus low-degree polynomials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YE21F_dBYU-",
        "colab_type": "text"
      },
      "source": [
        "## Similarity Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw8xFXG_BtxZ",
        "colab_type": "text"
      },
      "source": [
        "Another technique to tackle nonlinear problems is to add features computed using a **similarity function** which measures how much each instance resembles a particular **landmark**.\n",
        "\n",
        "One similarity function is the Gaussian Radial Basis Function (RBF).\n",
        "\n",
        "This is a bell-shaped function varying from 0 (very far from the landmark) to 1 (the landmark).\n",
        "\n",
        "To select the landmarks, the simplest approach is to createa landmark at the location of each and every instance in the dataset. Doing that creates many dimensions and thus increases the chances that the transformed training set will be linearly seperable. The downside is that a training set with m instances and n features will be transformed into a training set with m instances and m features (assuming that the original features have been dropped). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX81fs8hC7Dl",
        "colab_type": "text"
      },
      "source": [
        "## Guassian RBF Kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC8mWLv5DXDt",
        "colab_type": "text"
      },
      "source": [
        "The kernel trick does its SVM magic, making it possible to obtain a similar result as if you added many similarity features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKZoqdWRC_DN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rbf_kernel_svm_clf = Pipeline([\n",
        "                               ('scaler',StandardScaler()),\n",
        "                               (\"svm_clf\",SVC(kernel=\"rbf\",gamma=5,C=0.001))\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "040bft_BDP_Q",
        "colab_type": "code",
        "outputId": "d05b6746-3c15-4dd6-8eb9-f2f220ec0296",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "source": [
        "rbf_kernel_svm_clf.fit(X,y)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('scaler',\n",
              "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
              "                ('svm_clf',\n",
              "                 SVC(C=0.001, break_ties=False, cache_size=200,\n",
              "                     class_weight=None, coef0=0.0,\n",
              "                     decision_function_shape='ovr', degree=3, gamma=5,\n",
              "                     kernel='rbf', max_iter=-1, probability=False,\n",
              "                     random_state=None, shrinking=True, tol=0.001,\n",
              "                     verbose=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpSU4LHhDm7h",
        "colab_type": "text"
      },
      "source": [
        "The hyperparameter gamma:\n",
        "* Makes the bell-shaped curve narrower if increased.\n",
        "* The result is each instance's range of influence is smaller.\n",
        "\n",
        "Conversely, a small gamma value makes the bell-shaped curve wider: Instances have a larger range of influence, and the decision boundary ends up smoother.\n",
        "\n",
        "It acts like a regularization hyperparameter: if the model is overfitting, we should reduce it; if it's underfitting, we should increase it (likewise the hyperparameter C)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfz0lb5RXNv4",
        "colab_type": "text"
      },
      "source": [
        "Other kernels exist but they are used more rarely (like the ones specialized for specific data structures, like string kernels for documents or DNA sequences)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKGgheqvXdeH",
        "colab_type": "text"
      },
      "source": [
        "As a rule of thumb, it should be always tried first the LinearSVC (as it's faster than the SVC(kernel=\"linear\"), especially if the training set is very large or plenty of features. If it's not too large, the Gaussian RBF kernel can be also tried."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxiAD1I_Y-iO",
        "colab_type": "text"
      },
      "source": [
        "# Computational Complexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAcCPXBOL5_o",
        "colab_type": "text"
      },
      "source": [
        "The LinearSVC does not support the kernel trick, but it scales almost linearly with the number of training instances and the number of features. Its training time complexity is roughly O (m x n). The algorithm takes longer if you require very high precision. This is controlled by the tolerance hyperparameter tol in Scikit-Learn. In most classification, the default tolerance is fine.\n",
        "\n",
        "The SVC class can be dreadfully slow when the number of training instances gets large ( because training complexity time is between O (m^2 * n ) and (m^3 * n )).\n",
        "This algorithm is perfect for complex small or medium-sized training sets. It scales well with the number of features, especially with sparse features (when each instance has few nonzero features)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjBj5WOcND4p",
        "colab_type": "text"
      },
      "source": [
        "# SVM Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQgzJ6PBNG3W",
        "colab_type": "text"
      },
      "source": [
        "To use the SVMs for regression instead of classifications, the trick is to reverse the objective. Instead of trying to fit the largest possible street between two classes while limiting margin violations, SVM Regression tries to fit as many instances as possible on the street while limiting margin violations (instance off the street). The width of the street is controlled by a hyperparameter, tol.\n",
        "\n",
        "Adding more training instance within the margin does not affect the model's prediction; thus the model is said to be tol-insensitive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S63KOoOcDSDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import LinearSVR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWeD0lwYO8Eh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "97a03d5a-a4aa-40aa-f9d3-2939bba05ab7"
      },
      "source": [
        "svm_reg = LinearSVR(epsilon = 1.5)\n",
        "svm_reg.fit(X,y)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVR(C=1.0, dual=True, epsilon=1.5, fit_intercept=True,\n",
              "          intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
              "          random_state=None, tol=0.0001, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iLEuQv2PITc",
        "colab_type": "text"
      },
      "source": [
        "To tackle nonlinear regression tasks, you can use a kernelized SVM model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia0s3j_tPBbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reminder : Small C Value = High regularization, high C value = Low regularization."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb0RA_FrQABo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYi39iLfQGQv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "07b9df54-966e-4c5a-ebf9-e44bf2442147"
      },
      "source": [
        "svm_poly_reg = SVR(kernel=\"poly\",degree=2,C=100,epsilon=0.1)\n",
        "svm_poly_reg.fit(X,y)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVR(C=100, cache_size=200, coef0=0.0, degree=2, epsilon=0.1, gamma='scale',\n",
              "    kernel='poly', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VruLMeJhQXu7",
        "colab_type": "text"
      },
      "source": [
        " The SVR Class is the regression equivalent of the SVC class, and the LinearSVR is the equivalent of the LinearSVC Class. The LinearSVR class scales linearly with the size of the training set, while the SVR class gets too slow if the training set grows large ( just like the SVC class)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HweoHCLSQuy2",
        "colab_type": "text"
      },
      "source": [
        "*The SVMs can also be used for outliers detection.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdnhTDHuRRnl",
        "colab_type": "text"
      },
      "source": [
        "# Under the hood : Decision function and Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze_OkfwaRSTb",
        "colab_type": "text"
      },
      "source": [
        "The Linear SVM Classifier model predicts the class of a new instance **x** by simply computing the decision function **W(feature weights)^T X + b (bias term)**. If the result if positive, the predicted class is the positive class (1), if negative, the predicted class is the negative class (0).\n",
        "\n",
        "**Training a linear SVM classifier means finding the values of w and b that make this margin as wide as possible while avoiding margin violations (hard margin) or limiting margin (soft margin).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhhQC-eVTEle",
        "colab_type": "text"
      },
      "source": [
        "# Training Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E9BagRCTPd0",
        "colab_type": "text"
      },
      "source": [
        "Dividing the slope by 2 will multiply the margin by 2. The smaller the weight vector w, the larger the margin.\n",
        "\n",
        "We want to minimize ||w|| to get a larger margin but also avoid any margin violations (hard margin) then we need the decision function to be greater than 1 for all positive training instances and lower than -1 for negative training instances.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h54G2mNvVokK",
        "colab_type": "text"
      },
      "source": [
        "For the soft margin objective, we need to introduce a slack variable for each instance. We have now two conflicting objectives: make the slack variables as small as possble to reduce the margin violations, and make 1/2*W^T*W as small as possible to increase the margin. This is where the hyperparameter C comes in: it allows us to define the trade-off between these two objectives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTUNgyYZWnfV",
        "colab_type": "text"
      },
      "source": [
        "# Quadratic Programming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64Lnx0wyWpdB",
        "colab_type": "text"
      },
      "source": [
        "The hard margin and soft margin problems are both convex quadratic optimization problems with linear constraints. Such problems are known as Quadratic Programming (QP) problems.\n",
        "\n",
        "One way to train a hard margin linear SVM classifier is to use an off-the-shelf QP solver and pass it the preceding parameters.\n",
        "\n",
        "To use the kernel trick, we are going to look at a different constrained optimization problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlI-YaY4W9_i",
        "colab_type": "text"
      },
      "source": [
        "# The Dual Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O50ONvXEYJ8b",
        "colab_type": "text"
      },
      "source": [
        "Given a constrained optimization problem, known as the primal problem, it is possible to express a different but closely related problem, called its dual problem. \n",
        "\n",
        "The dual problem is fasterto solve than the primal one when the number of training instances is smaller than the number of features and makes the kernel trick possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRWWSEe8Yjzl",
        "colab_type": "text"
      },
      "source": [
        "# Kernelized SVMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwQyMM2qYk0D",
        "colab_type": "text"
      },
      "source": [
        "In Machine-Learning, a *kernel* is a function capable of computing the dot product (/)(a)^T * (/)(b) based only on the original vectors a and b, without having to compute the transformations (/). Some of the most common kernels are linear, polynomial, gaussian RBF, sigmoid."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMdqyNRydN7Z",
        "colab_type": "text"
      },
      "source": [
        "# Online SVMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDRTawiJdN36",
        "colab_type": "text"
      },
      "source": [
        "As a reminder, online learning means learning incrementally, typically as new instances arrive.\n",
        "\n",
        "For linear SVM Classifiers,one method for implementing an online SVM classifier is to use Gradient Descent (using SGDClassifier) to minimize the cost function which is derived from the primal proble. Unfortunately, Gradient Descent converges much more slowly than the methods based on QP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NAr-Xr5fTbs",
        "colab_type": "text"
      },
      "source": [
        "For large-scale nonlinear problems, you may want to consider using neural networks instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_sYyRTOffKs",
        "colab_type": "text"
      },
      "source": [
        "**Hinge Loss** : The function max(0,1 - t) is called the *hinge loss* function and it is equal to zero when t>=1. Its derivative is equal to -1 if t <1 and 0 if t >1. It is not differentiable at t=1, but just like Lasso Regression, we can use Gradient Descent using any subderivative at t=1 (any value between -1 and 0)."
      ]
    }
  ]
}