{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Which Linear Regression training algorithm can you use if you have a training set with millions of features?\n",
    "    * We can use Lasso Regression algorithm to do some feature selection (because the algorithm will put useless features' weight to zero). We can also use ElasticNet which is a mix between Lasso Regression and Ridge Regression.\n",
    "    \n",
    "    \n",
    "2. Suppose the features in your training set have very different scales. What algorithms might suffer from this, and how? What can you do about it?\n",
    "    * Every regularized algorithm will suffer from having data with different scales because they are sensitive to the scale of the input features. They will behave erractly by removing or reducing / increasing the weights of those features. What we can do about it is to standardize or normalize the data.\n",
    "    \n",
    "    \n",
    "3. Can Gradient Descent get stuck in a local minimum when training a Logistic Regression model?\n",
    "    * Gradient Descent cannot get stuck in a local minimum because the Logistic Regression model cost function is convex, and will reach directly the global minimum.\n",
    "    \n",
    "\n",
    "4. Do all Gradient Descent algorithms lead to the same model provided you let them run long enough?\n",
    "    * They will all converge towards the global minimum even though mini-batch GB and Stochastic GB will bounce around it. Only Batch Gradient reach the global minimum.\n",
    "    \n",
    "\n",
    "5. Suppose you use Batch Gradient Descent and you plot the validation error at every epoch. If you notice that the validation error consistently goes up, what is likely going on? How can you fix this?\n",
    "    * if the validation error goes up at every epoch, we are more likely to be overfitting the training data and therefore our model is not capable of generalizing correctly. We can fix this by adding more training data which will bring the validation and training error closer.\n",
    "    \n",
    "\n",
    "6. Is it a good idea to stop Mini-batch Gradient Descent immediately when the validation error goes up?\n",
    "    * No because it could be that the algorithm is getting out of a local minimum to reach the global minimum.\n",
    "    \n",
    "\n",
    "7. Which Gradient Descent algorithm (among those we discussed) will reach the vicinity of the optimal solution the fastest? Which will actually converge? How can you make the others converge as well?\n",
    "    * The Stochastic Gradient Descend is the one reaching the vicinity of the optimal solution the fastest, but will just converge like the Mini-Batch GD whereas the Batch GD will reach the global minimum. One solution to make them converge towards the global minimum is using a technique called *Simulated annealing* or learning Schedule where with start with a high learning rate and we reduce it little by little.\n",
    "    \n",
    "\n",
    "8. Suppose you are using Polynomial Regression. You plot the learning curves and you notice that there is a large gap between the training error and the validation error. What is happening? What are three ways to solve this?\n",
    "    * We are experiencing overfitting. The three ways to solve this are: Regularization, more training data and early stopping.\n",
    "    \n",
    "\n",
    "9. Suppose you are using Ridge Regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the model suffers from high bias or high variance? Should you increase the regularization hyperparameter Î± or reduce it?\n",
    "    * By having a training and validation error almost equal and fairly we can say that the model is underfitting and suffers from a high bias and a low variance. To reduce the the high bias, we should reduce the regularization hyperparameter.\n",
    "    \n",
    "\n",
    "10. Why would you want to use:\n",
    "    * Ridge Regression instead of Linear Regression? It's always better to use a bit of regulation than nothing (ridge versus linear)\n",
    "    * Lasso instead of Ridge Regression? When we have a high number of features and we want to withdraw the useless features.\n",
    "    * Elastic Net instead of Lasso? When we have a hyperparametric dataset (more features than observations) because Lasso may behave erratically or when the features are strongly correlated.\n",
    "    \n",
    "\n",
    "11. Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime. Should you implement two Logistic Regression classifiers or one Softmax Regression classifier?\n",
    "    * Since Softmax Regression classifier doesn't do multilabelling, we should use two logisitc regression classifiers since outdoor/indoor and daytime/nighttime are not mutually exclusive.\n",
    "\n",
    "\n",
    "12. Implement Batch Gradient Descent with early stopping for Softmax Regression (without using Scikit-Learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercice 12\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "X = iris[\"data\"] \n",
    "y = iris[\"target\"]\n",
    "\n",
    "\n",
    "scaler = Pipeline([\n",
    "#(\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
    "(\"std_scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "X_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "\n",
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\",C=10,warm_start=True)\n",
    "\n",
    "minimum_val_error = float(\"inf\")\n",
    "best_epoch = None\n",
    "best_model = None\n",
    "\n",
    "i = 0\n",
    "\n",
    "for epoch in range(1000):\n",
    "    \n",
    "    softmax_reg.fit(X_train,y_train)\n",
    "    y_val_predict = softmax_reg.predict_proba(X_val)\n",
    "    val_error = log_loss(y_val,y_val_predict)\n",
    "    \n",
    "    if val_error < minimum_val_error:\n",
    "        minimum_val_error = val_error\n",
    "        best_epoch = epoch\n",
    "        best_model = clone(softmax_reg)\n",
    "        \n",
    "    elif val_error >= minimum_val_error:\n",
    "        i +=1\n",
    "        if i == 3:\n",
    "            break\n",
    "    \n",
    "    \n",
    "        \n",
    "print(best_model)\n",
    "print(best_epoch)\n",
    "print(minimum_val_error)s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
