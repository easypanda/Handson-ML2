{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.If you have trained five different models on the exact same training data, and they all achieve 95% precision, is there any chance that you can combine these models to get better results? If so, how? If not, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have trained five different models on the exact same training data, and they all achieve 5% precision, is it possible to improve the results by combining these models. Since each model makes its own prediction,therefore it has its own caratheristics and it is possible that a model would perform better than the others to spot a certain pattern where the other models would perform better for the other patterns for instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.What is the difference between hard and soft voting classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hard voting classifier : The mode value (which is the most recurring value) or the mean among all the classifier is selected as value for the combined models. In other words, it is the aggregation of the predictions of each classifier and the predicted the class that gets the most votes.\n",
    "* Soft voting classifier : Only possible with classifiers that can compute the estimate class probabilities, soft voting classifiers will select the class with the highest class probability, averaged over all the individual classifiers. It often achieves better results than hard voting classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, random forests, or stacking ensembles?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, it it possible the distribute the training of bagging and pasting ensembles, and also to random forests and stacking ensembles. It is possible because all the trees are computed separatly and  of each others. In the other hand, it is not possible ( in theory) for boosting ensembles the weights of each trees depends of the results of the precedent tree, it is an iterative process which makes it not possible to compute accross multiple servers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. What is the benefit of out-of-bag evaluation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out-of-Bag evaluation allows the the unseen data (since only ~63% of the data are sampled on average because of the remplacement) of a bagging classifier to be used as a validation set for each predictors. Because all the predictors are not the same, it is possible to evaluate the ensemble by averaging the oob evaluations of each predictor, without the need for a separate validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. What makes Extra-Trees more random than regular Random Forests? How can this extra randomness help? Are Extra-Trees slower or faster than regular Random Forests?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests will try to find the best threshold possible to split the features where the Extra-Trees will split the features at random. This extra randomness will help to reduce the variance (but will trade it against some bias). This extra randomness also help the Extra-Trees to be faster to train than Random Forests as finding the best thresholds for the features is the most consuming tasks of the Random Forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. If your AdaBoost ensemble underfits the training data, what hyperparameters should you tweak and how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a AdaBoost ensemble is underfit, it means that the regularization of the model is too high or we have too little estimators, so we should increase the learning rate hyperparameter or increase the number of estimators (n_estimators), or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. If your Gradient Boosting ensemble overfits the training set, should you increase or decrease the learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the Gradient Boosting ensemble overfits the training set, we should decrease the learning rate as it would then require more trees to be trained to better fit the data. This regularization technique is called *Shrinkage*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Load the MNIST data (introduced in Chapter 3), and split it into a training set, a validation set, and a test set (e.g., use the first 40,000 instances for training, the next 10,000 for validation, and the last 10,000 for testing). Then train various classifiers, such as a Random Forest classifier, an Extra-Trees classifier, and an SVM. Next, try to combine them into an ensemble that outperforms them all on the validation set, using a soft or hard voting classifier. Once you have found one,try it on the test set. How much better does it perform compared to the individual classifiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the librairies\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "#Only for jupyter notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True) #Importing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(list(zip(X,y))) #Shuffling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data\n",
    "X_train, y_train = X[:40000],y[:40000] #40 000 instances for the train set\n",
    "X_val, y_val = X[40000:55000],y[40000:55000] #15 000 for the validation set\n",
    "X_test, y_test = X[55000:70000],y[55000:70000] #15 000 for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training set (40000, 784)\n",
      "Shape of the validation set (15000, 784)\n",
      "Shape of the test set (15000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of the training set {X_train.shape}\")\n",
    "print(f\"Shape of the validation set {X_val.shape}\")\n",
    "print(f\"Shape of the test set {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the algorithms\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC,LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialization\n",
    "rf_clf = RandomForestClassifier(n_jobs = -1,random_state = 42)\n",
    "extraTrees_clf = ExtraTreesClassifier(n_jobs = -1,random_state = 42)\n",
    "svm_clf = LinearSVC(random_state = 42)\n",
    "lr_clf = LogisticRegression(n_jobs = -1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [rf_clf,extraTrees_clf,svm_clf,lr_clf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
      "<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>\n",
      "<class 'sklearn.svm._classes.LinearSVC'>\n",
      "<class 'sklearn.linear_model._logistic.LogisticRegression'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "for estimator in estimators:\n",
    "    print(estimator.__class__)\n",
    "    estimator.fit(X_train,y_train) #Fitting the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the hard and soft voting classifiers\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score #Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=-1, oob_score=False, random_state=42, verbose=0,\n",
      "                       warm_start=False) 0.963\n",
      "ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
      "                     criterion='gini', max_depth=None, max_features='auto',\n",
      "                     max_leaf_nodes=None, max_samples=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "                     oob_score=False, random_state=42, verbose=0,\n",
      "                     warm_start=False) 0.9649333333333333\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=42, tol=0.0001,\n",
      "          verbose=0) 0.861\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=-1, penalty='l2', random_state=42,\n",
      "                   solver='lbfgs', tol=0.0001, verbose=0, warm_start=False) 0.9112666666666667\n"
     ]
    }
   ],
   "source": [
    "accuracy_models = []\n",
    "for estimator in estimators:\n",
    "    y_pred = estimator.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val,y_pred)\n",
    "    accuracy_models.append(accuracy)\n",
    "\n",
    "for estimator,accuracy in zip(estimators, accuracy_models):\n",
    "    print(estimator,accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hard 0.9625333333333334\n",
      "soft 0.9485333333333333\n"
     ]
    }
   ],
   "source": [
    "accuracy_voting = []\n",
    "\n",
    "voting = [\"hard\",\"soft\"] #For the voting classifier\n",
    "\n",
    "for vote in voting:\n",
    "    voting_clf = VotingClassifier(\n",
    "            estimators=[(\"rf\",rf_clf),(\"extratrees\",extraTrees_clf),(\"lr\",lr_clf)],\n",
    "            voting=vote,\n",
    "            n_jobs=-1)\n",
    "    voting_clf.fit(X_train,y_train)\n",
    "    y_pred = voting_clf.predict(X_val)\n",
    "    accuracy_vote = accuracy_score(y_val,y_pred)\n",
    "    accuracy_voting.append(accuracy_vote)\n",
    "    \n",
    "for vote,accuracy in zip(voting,accuracy_voting):\n",
    "    print(vote, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators.append(voting_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=-1, oob_score=False, random_state=42, verbose=0,\n",
      "                       warm_start=False)\n",
      "ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
      "                     criterion='gini', max_depth=None, max_features='auto',\n",
      "                     max_leaf_nodes=None, max_samples=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "                     oob_score=False, random_state=42, verbose=0,\n",
      "                     warm_start=False)\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=42, tol=0.0001,\n",
      "          verbose=0)\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=-1, penalty='l2', random_state=42,\n",
      "                   solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)\n",
      "VotingClassifier(estimators=[('rf',\n",
      "                              RandomForestClassifier(bootstrap=True,\n",
      "                                                     ccp_alpha=0.0,\n",
      "                                                     class_weight=None,\n",
      "                                                     criterion='gini',\n",
      "                                                     max_depth=None,\n",
      "                                                     max_features='auto',\n",
      "                                                     max_leaf_nodes=None,\n",
      "                                                     max_samples=None,\n",
      "                                                     min_impurity_decrease=0.0,\n",
      "                                                     min_impurity_split=None,\n",
      "                                                     min_samples_leaf=1,\n",
      "                                                     min_samples_split=2,\n",
      "                                                     min_weight_fraction_leaf=0.0,\n",
      "                                                     n_estimators=100,\n",
      "                                                     n_jobs=-1, oob_score=F...\n",
      "                                                   oob_score=False,\n",
      "                                                   random_state=42, verbose=0,\n",
      "                                                   warm_start=False)),\n",
      "                             ('lr',\n",
      "                              LogisticRegression(C=1.0, class_weight=None,\n",
      "                                                 dual=False, fit_intercept=True,\n",
      "                                                 intercept_scaling=1,\n",
      "                                                 l1_ratio=None, max_iter=100,\n",
      "                                                 multi_class='auto', n_jobs=-1,\n",
      "                                                 penalty='l2', random_state=42,\n",
      "                                                 solver='lbfgs', tol=0.0001,\n",
      "                                                 verbose=0,\n",
      "                                                 warm_start=False))],\n",
      "                 flatten_transform=True, n_jobs=-1, voting='soft',\n",
      "                 weights=None)\n",
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'> 0.969\n",
      "<class 'sklearn.ensemble._forest.ExtraTreesClassifier'> 0.9717333333333333\n",
      "<class 'sklearn.svm._classes.LinearSVC'> 0.8782666666666666\n",
      "<class 'sklearn.linear_model._logistic.LogisticRegression'> 0.9279333333333334\n",
      "<class 'sklearn.ensemble._voting.VotingClassifier'> 0.9579333333333333\n"
     ]
    }
   ],
   "source": [
    "accuracy_models = []\n",
    "for estimator in estimators:\n",
    "    print(estimator)\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    accuracy_models.append(accuracy)\n",
    "\n",
    "for estimator,accuracy in zip(estimators, accuracy_models):\n",
    "    print(estimator.__class__,accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The voting classifier does slightly better ! (with a svm, not a LR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Run the individual classifiers from the previous exercise to make predictions on the validation set, and create a new training set with the resulting predictions: each training instance is a vector containing the set of predictions from all your classifiers for an image, and the target is the image’s class. Congratulations, you have just trained a blender, and together with the classifiers they form a stacking ensemble! Now let’s evaluate the ensemble on the test set. For each image in the test set, make predictions with all your classifiers, then feed the predictions to the blender to get the ensemble’s predictions. How does it compare to the voting classifier you trained earlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators.remove(voting_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_prediction = np.empty((len(X_val),len(estimators)),dtype = np.float32) # Create an empty array for our next prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, model in enumerate(estimators):\n",
    "    X_val_prediction[:,index] = model.predict(X_val) #Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7., 7., 7., 7.],\n",
       "       [7., 7., 7., 7.],\n",
       "       [0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0.],\n",
       "       [4., 4., 9., 4.],\n",
       "       [0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_blender = RandomForestClassifier(oob_score=True,n_jobs = -1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=-1, oob_score=True, random_state=42, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_blender.fit(X_val_prediction,y_val) #Using the prediction done as input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9618"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_blender.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stacking_clf = StackingClassifier(\n",
    "    estimators = [(\"rf\",rf_clf),(\"extratrees\",extraTrees_clf),(\"LinearSVM\",svm_clf),(\"Lr_clf\",lr_clf)],\n",
    "    final_estimator = LogisticRegression(),\n",
    "    n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StackingClassifier(cv=None,\n",
       "                   estimators=[('rf',\n",
       "                                RandomForestClassifier(bootstrap=True,\n",
       "                                                       ccp_alpha=0.0,\n",
       "                                                       class_weight=None,\n",
       "                                                       criterion='gini',\n",
       "                                                       max_depth=None,\n",
       "                                                       max_features='auto',\n",
       "                                                       max_leaf_nodes=None,\n",
       "                                                       max_samples=None,\n",
       "                                                       min_impurity_decrease=0.0,\n",
       "                                                       min_impurity_split=None,\n",
       "                                                       min_samples_leaf=1,\n",
       "                                                       min_samples_split=2,\n",
       "                                                       min_weight_fraction_leaf=0.0,\n",
       "                                                       n_estimators=100,\n",
       "                                                       n_jobs=-1,\n",
       "                                                       o...\n",
       "                                                   solver='lbfgs', tol=0.0001,\n",
       "                                                   verbose=0,\n",
       "                                                   warm_start=False))],\n",
       "                   final_estimator=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                      dual=False,\n",
       "                                                      fit_intercept=True,\n",
       "                                                      intercept_scaling=1,\n",
       "                                                      l1_ratio=None,\n",
       "                                                      max_iter=100,\n",
       "                                                      multi_class='auto',\n",
       "                                                      n_jobs=None, penalty='l2',\n",
       "                                                      random_state=None,\n",
       "                                                      solver='lbfgs',\n",
       "                                                      tol=0.0001, verbose=0,\n",
       "                                                      warm_start=False),\n",
       "                   n_jobs=-1, passthrough=False, stack_method='auto',\n",
       "                   verbose=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Stacking_clf.fit(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = Stacking_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
