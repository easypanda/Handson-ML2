{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What are the main motivations for reducing a dataset’s dimensionality? What are the main drawbacks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main motivations for reducing a dataset's dimensionality are the following:\n",
    "* Speed up the training process, as the number of dimensionality is lower.\n",
    "* Filtering out some noise inside the dataset.\n",
    "* DataViz purposes (finding patterns, presenting results to a non technical audience,etc.) where the dataset has to be represent into a chart, where the lower the number of dimensionality, the better it is.\n",
    "\n",
    "The main drawbacks are:\n",
    "* The loss of information inside the dataset and thus the (slight) drop in performance in the model.\n",
    "* The increase complexity of the pipeline because of the dimensionality reduction and therefore the difficulty to maintain it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. What is the curse of dimensionality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning problems involve thousands or even millions of features for each training instance. Not only do all these features make training extremely slow, but they can make it harder to find a good solution. This problem is ofter referred to as the ***curse of dimensionality***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Once a dataset’s dimensionality has been reduced, is it possible to reverse the operation? If so, how? If not, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, once a dataset's dimensionality has been reduced, it is possible to reserve the operation. Since the PCA uses projections to reduce dimensionality, it is possible to get the inverse transformation to get back the original dataset. However, this won't give us back the original data, since the projection lost a bit of information (within the % variance that was dropped), but it will likely be close to the original data. The mean squared distance between the original data and the reconstructed data (compressed than decompressed) is called the *reconstruction error*.\n",
    "In scikit-learn, the operation is done with the *inverse_transform* class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, it is possible with a kernel PCA (kPCA). This technique will map instance in the *feature space* enabling nonlinear decision boundary in the *original space* to be a linear decision boundary in a the *feature space*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained variance ratio to 95%. How many dimensions will the resulting dataset have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting dataset will have <1000 dimensions and will have n-dimension that represents 95% of the variance explained in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. In what cases would you use vanilla PCA, Incremental PCA, Randomized PCA,or Kernel PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vanilla PCA:\n",
    "* When the dimensionality of a dataset is too high, which makes the training too slow.\n",
    "\n",
    "Incremental PCA:\n",
    "* When the dataset is too big to fit on the memory and to make online prediction,on the fly, when new training instances arrive, because the it will split the training set into mini-batches.\n",
    "\n",
    "Randomized PCA:\n",
    "* To quickly find an approximation of the first d principal components.\n",
    "\n",
    "Kernel PCA:\n",
    "* When the dataset is highly nonlinear, and when we want to preserve clusters after projections, or even unfold twisted manifold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. How can you evaluate the performance of a dimensionality reduction algorithm on your dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction is often a preparation step for a supervised learning task (e.g. classification), so we can use grid search to select the kernel and hyperparameters that lead to the best performance on the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Does it make any sense to chain two different dimensionality reduction algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, we might lose too much information between the two dimensionality reduction algorithms. The reconstruction error might be too high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Load the MNIST dataset (introduced in Chapter 3) and split it into a training set and a test set (take the first 60,000 instances for training, and the remaining 10,000 for testing). Train a Random Forest classifier on the dataset and time how long it takes, then evaluate the resulting model on the test set. Next, use PCA to reduce the dataset’s dimensionality, with an explained variance ratio of 95%. Train a new Random Forest classifier on the reduced dataset and see how long it takes. Was training much faster? Next evaluate the classifier on the test set: how does it compare to the previous classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import algorithms\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Import dataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "#Import train test split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True) #Importing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed (hh:mm:ss.ms) 0:00:18.267827\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now() \n",
    "rf_clf.fit(X_train,y_train) \n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(datetime.now() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9714"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(rf_clf.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed (hh:mm:ss.ms) 0:00:35.645080\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now() \n",
    "rf_clf.fit(X_pca,y_train)\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(datetime.now() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9493"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(rf_clf.predict(pca.transform(X_test)),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is slightly worse with the PCA (2.1% difference), but the training speed has been decrease by 84%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Use t-SNE to reduce the MNIST dataset down to two dimensions and plot the result using Matplotlib. You can use a scatterplot using 10 different colors to represent each image’s target class. Alternatively, you can write colored digits at the location of each instance, or even plot scaled-down versions of the digit images themselves (if you plot all digits, the visualization will be too cluttered, so you should either draw a random sample or plot an instance only if no other instance has already been plotted at a close distance). You should get a nice visualization with well-separated clusters of digits. Try using other dimensionality reduction algorithms such as PCA, LLE, or MDS and compare the resulting visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import t-SNE\n",
    "\n",
    "from sklearn.manifold import TSNE,MDS, LocallyLinearEmbedding\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "pca = PCA(n_components=2)\n",
    "mds = MDS(n_components=2)\n",
    "lle = LocallyLinearEmbedding(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = np.random.choice(X.shape[0],size=10000) #random split of the dataset to speed up the process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 784), (10000,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[split].shape,y[split].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne = tsne.fit_transform(X[split])\n",
    "X_pca = pca.fit_transform(X[split])\n",
    "X_mds = mds.fit_transform(X[split])\n",
    "X_lle = lle.fit_transform(X[split])\n",
    "y_1000 = y[split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_viz = [X_tsne,X_pca,X_mds,X_lle]\n",
    "list_viz_str = [\"tsne\",\"pca\",\"mds\",\"lle\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in zip(list_viz,list_viz_str):\n",
    "    data = pd.concat([pd.DataFrame(i).rename({0:\"X\",1:\"Y\"},axis=1),pd.DataFrame(y_1000).rename({0:\"digit\"},axis=1)],axis=1)\n",
    "    plt.figure(figsize=(15,10))\n",
    "    sns.scatterplot(x=\"X\",\n",
    "                    y=\"Y\",\n",
    "                    hue=\"digit\",\n",
    "                    data=data,\n",
    "                    legend=\"full\",\n",
    "                    palette=\"muted\")\n",
    "    plt.title(f\"MNIST {str.upper(j)} REDUCED DATASET\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unique(y_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for data,target in zip(X_tsne,y_tsne):\n",
    "#    print(data,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = plt.figure(figsize=(15,10))\n",
    "#ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "\n",
    "#for data,target in zip(X_tsne,y_tsne):\n",
    "#    \n",
    "#    ax.scatter( data[0],\n",
    "#                data[1])\n",
    "    \n",
    "    #ax.scatter(grades_range, girls_grades, color='r')\n",
    "    #ax.scatter(grades_range, boys_grades, color='b')\n",
    "#plt.title(\"MNIST T-SNE REDUCED DATASET\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
