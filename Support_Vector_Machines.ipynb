{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Support Vector Machines.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMRsr9VUJ8g7/AAHQD4UEFy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/easypanda/Handson-ML2/blob/master/Support_Vector_Machines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ai_gUE_EV69",
        "colab_type": "text"
      },
      "source": [
        "# TO BE FINISHED"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDgyowoK2GV4",
        "colab_type": "text"
      },
      "source": [
        "#Support Vector Machines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09Bz-S4K1jFq",
        "colab_type": "text"
      },
      "source": [
        "A Support Vector Machine (SVM) is a powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression , and even outlier detection. SVMs are particulary well suited for classification of complex small- or medium-sized datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2-5qrqX2LDC",
        "colab_type": "text"
      },
      "source": [
        "## Linear SVM Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-fABYsD2oHo",
        "colab_type": "text"
      },
      "source": [
        "You can think of an SVM classifier as fitting the widest possible street( represented by the parallel dashed lines) between the classes. This is called **large margin classification**. Notice that adding more training instances \"off the street\" will not affect the decision boundary at all: it is fully determined by the instances located on the edge of the street. These instances are called the **Support Vectors**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_kdbwIr3sFP",
        "colab_type": "text"
      },
      "source": [
        "/!\\ : SVMs are sensible to the feature scales so we should do feature scaling each time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwFqD-ou36CU",
        "colab_type": "text"
      },
      "source": [
        "## Soft Margin Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-oBxRMg4s6k",
        "colab_type": "text"
      },
      "source": [
        "If we strictly impose that all instances must be off the street and on the right side, this is called **hard margin classification**.\n",
        "This has two main issues:\n",
        "* It only works if the data are linearly seperable.\n",
        "* It is sensitive to outliers.\n",
        "\n",
        "We choose use a more flexible model that ensure the good balance between keeping the street as large as possible and limiting the *margin violations*.\n",
        "This is called **Soft Margin Classification**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l-bpgqK5ykc",
        "colab_type": "text"
      },
      "source": [
        "When creating a model with Scikit-Learn, we can specify the number of hyperparameters including C. if C is set to a low value, we enable a large margin and therefore more margin violations whereas with a high C, a fewer margin violations. Sometimes it's more sutable to have more margin violations to ensure a better generalization of the model.\n",
        "\n",
        "/!\\ If the model is overfitting, we can try regularizing by reducing C."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uV1WOs0D25G1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvYP8SG_6vqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iris = datasets.load_iris()\n",
        "X = iris[\"data\"][:,(2,3)] #Petal length and petal width columns\n",
        "y = (iris[\"target\"] == 2).astype(np.float64) #Just Iris virginica"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWmEeboq7F3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svm_clf = Pipeline([\n",
        "                    (\"scaler\",StandardScaler()),\n",
        "                    (\"linear_svc\",LinearSVC(C=1,loss=\"hinge\")),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MBwnNPH7QT2",
        "colab_type": "code",
        "outputId": "6c84818a-0e43-409f-fc37-41ce6199666f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "svm_clf.fit(X,y)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('scaler',\n",
              "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
              "                ('linear_svc',\n",
              "                 LinearSVC(C=1, class_weight=None, dual=True,\n",
              "                           fit_intercept=True, intercept_scaling=1,\n",
              "                           loss='hinge', max_iter=1000, multi_class='ovr',\n",
              "                           penalty='l2', random_state=None, tol=0.0001,\n",
              "                           verbose=0))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqnwhF6Z7Sxf",
        "colab_type": "code",
        "outputId": "f5570e68-859d-4ea6-8a9f-a8155cb81580",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "svm_clf.predict([[5.5,1.7]])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2YTs1ce7Zt1",
        "colab_type": "text"
      },
      "source": [
        "**/!\\** Unline Logistic Regression, SVM does not output probabilities for each class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKnnmew39RIR",
        "colab_type": "text"
      },
      "source": [
        "We could also use the SVC class with a liner kernel (SVC(kernel=\"linear\",C=1)) or with the SGDClassifier (SGDClassifier(loss=\"hinge\",alpha=1/(m*c))). It does not converge as fast as the LinearSVC class but it can be useful to handle online classification tasks or huge datasets that do not fit in memory (out-of-core training).\n",
        "\n",
        "Loss hyperparameter should be on \"hinge\" and for better performance, dual hyperparameter should be set on False."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbbpZZGn9-Oq",
        "colab_type": "text"
      },
      "source": [
        "## Nonlinear SVM Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7WJugV9-WE4",
        "colab_type": "text"
      },
      "source": [
        "One approche to handle nonlinear datasets is to add more features, such as polynomial features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBDPFBEk7W6o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TROb5quD-fdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X,y = make_moons(n_samples=100,noise=0.15)\n",
        "polynomial_svm_clf = Pipeline([\n",
        "                               ('poly_features',PolynomialFeatures(degree=3)),\n",
        "                               ('scaler',StandardScaler()),\n",
        "                               ('svm_clf',LinearSVC(C=10,loss=\"hinge\"))\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRQnRwIM_P2u",
        "colab_type": "code",
        "outputId": "d560367f-06fd-4bb9-e7be-0b3c330d6a04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "polynomial_svm_clf.fit(X,y)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('poly_features',\n",
              "                 PolynomialFeatures(degree=3, include_bias=True,\n",
              "                                    interaction_only=False, order='C')),\n",
              "                ('scaler',\n",
              "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
              "                ('svm_clf',\n",
              "                 LinearSVC(C=10, class_weight=None, dual=True,\n",
              "                           fit_intercept=True, intercept_scaling=1,\n",
              "                           loss='hinge', max_iter=1000, multi_class='ovr',\n",
              "                           penalty='l2', random_state=None, tol=0.0001,\n",
              "                           verbose=0))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZdZSsIk_V1P",
        "colab_type": "text"
      },
      "source": [
        "## Polynomial Kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzdNdCNS_9ux",
        "colab_type": "text"
      },
      "source": [
        "Adding a polynomial features is simple to implement and can work great with all sorts of Machine Learning algorithms (not just SVM). That said, at a low polynomial degree, this method cannot deal with very complex datasets, and with a high polynomial degree it creates a huge number of features, making the model too slow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he-cdHouABwh",
        "colab_type": "text"
      },
      "source": [
        "But with SVM, you can apply a technique called **kernel trick** to make possible to have the same result as if you had added many polynomial features, even with very high-degree polynomials without actually adding them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFgTdB1y_STf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2JzZ_MvAXOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "poly_kernel_svm_clf = Pipeline([\n",
        "                                (\"scaler\",StandardScaler()),\n",
        "                                (\"svm_clf\",SVC(kernel=\"poly\",degree=3,coef0=1,C=5))\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDiWl2p-Alrf",
        "colab_type": "code",
        "outputId": "cf32e932-4de4-4147-f867-94c8ab985309",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "poly_kernel_svm_clf.fit(X,y)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('scaler',\n",
              "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
              "                ('svm_clf',\n",
              "                 SVC(C=5, break_ties=False, cache_size=200, class_weight=None,\n",
              "                     coef0=1, decision_function_shape='ovr', degree=3,\n",
              "                     gamma='scale', kernel='poly', max_iter=-1,\n",
              "                     probability=False, random_state=None, shrinking=True,\n",
              "                     tol=0.001, verbose=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIB55tBJBOFV",
        "colab_type": "text"
      },
      "source": [
        "This SVM classifier uses a third-degree polynomial kernel.\n",
        "If the model is overfitting, we can decrease the polynomial degree and if it's underfitting we can increase it.\n",
        "The hyperparemeter coef0 controls how much the model is unfluenced by high-degree polynomials versus low-degree polynomials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YE21F_dBYU-",
        "colab_type": "text"
      },
      "source": [
        "## Similarity Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw8xFXG_BtxZ",
        "colab_type": "text"
      },
      "source": [
        "Another technique to tackle nonlinear problems is to add features computed using a **similarity function** which measures how much each instance resembles a particular **landmark**.\n",
        "\n",
        "One similarity function is the Gaussian Radial Basis Function (RBF).\n",
        "\n",
        "This is a bell-shaped function varying from 0 (very far from the landmark) to 1 (the landmark).\n",
        "\n",
        "To select the landmarks, the simplest approach is to createa landmark at the location of each and every instance in the dataset. Doing that creates many dimensions and thus increases the chances that the transformed training set will be linearly seperable. The downside is that a training set with m instances and n features will be transformed into a training set with m instances and m features (assuming that the original features have been dropped). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX81fs8hC7Dl",
        "colab_type": "text"
      },
      "source": [
        "## Guassian RBF Kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC8mWLv5DXDt",
        "colab_type": "text"
      },
      "source": [
        "The kernel trick does its SVM magic, making it possible to obtain a similar result as if you added many similarity features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKZoqdWRC_DN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rbf_kernel_svm_clf = Pipeline([\n",
        "                               ('scaler',StandardScaler()),\n",
        "                               (\"svm_clf\",SVC(kernel=\"rbf\",gamma=5,C=0.001))\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "040bft_BDP_Q",
        "colab_type": "code",
        "outputId": "8b864d08-6667-47a5-fa77-56702889f2a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "source": [
        "rbf_kernel_svm_clf.fit(X,y)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('scaler',\n",
              "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
              "                ('svm_clf',\n",
              "                 SVC(C=0.001, break_ties=False, cache_size=200,\n",
              "                     class_weight=None, coef0=0.0,\n",
              "                     decision_function_shape='ovr', degree=3, gamma=5,\n",
              "                     kernel='rbf', max_iter=-1, probability=False,\n",
              "                     random_state=None, shrinking=True, tol=0.001,\n",
              "                     verbose=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpSU4LHhDm7h",
        "colab_type": "text"
      },
      "source": [
        "The hyperparameter gamma:\n",
        "* Makes the bell-shaped curve narrower if increased.\n",
        "* The result is each instance's range of influence is smaller.\n",
        "\n",
        "Conversely, a small gamma value makes the bell-shaped curve wider: Instances have a larger range of influence, and the decision boundary ends up smoother.\n",
        "\n",
        "It acts like a regularization hyperparameter: if the model is overfitting, we should reduce it; if it's underfitting, we should increase it (likewise the hyperparameter C)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfz0lb5RXNv4",
        "colab_type": "text"
      },
      "source": [
        "Other kernels exist but they are used more rarely (like the ones specialized for specific data structures, like string kernels for documents or DNA sequences)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKGgheqvXdeH",
        "colab_type": "text"
      },
      "source": [
        "As a rule of thumb, it should be always tried first the LinearSVC (as it's faster than the SVC(kernel=\"linear\"), especially if the training set is very large or plenty of features. If it's not too large, the Gaussian RBF kernel can be also tried."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxiAD1I_Y-iO",
        "colab_type": "text"
      },
      "source": [
        "# Computational Complexity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S63KOoOcDSDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}